[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Vincenzo Palazeti",
    "section": "",
    "text": "My name is Vincenzo Palazeti, but everyone calls me Vinnie.\nI am a Data Scientist at Vevo. My main focus thus far has been forecasting various quantities. Views, Watch Time, Revenue, etc.\nPrior to Vevo I worked in the Sports & Horse wagering industries. Some of the topics I’ve spent time on are yield optimization, ensembling methods, ranking algorthims, timeseries, experimental methods, & data engineering."
  },
  {
    "objectID": "about.html#hello",
    "href": "about.html#hello",
    "title": "Vincenzo Palazeti",
    "section": "",
    "text": "My name is Vincenzo Palazeti, but everyone calls me Vinnie.\nI am a Data Scientist at Vevo. My main focus thus far has been forecasting various quantities. Views, Watch Time, Revenue, etc.\nPrior to Vevo I worked in the Sports & Horse wagering industries. Some of the topics I’ve spent time on are yield optimization, ensembling methods, ranking algorthims, timeseries, experimental methods, & data engineering."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Vincenzo Palazeti",
    "section": "Experience",
    "text": "Experience\n\n\n Data Scientist, 2023-Present\n\n\nVevo\n\n\n Data Scientist, 2022-2023\n\n\nAlphaPeak\n\n\n Data Scientist, 2020-2022\n\n\nSports Betting Innovative Analytics\n\n\n Statistician, 2019-2020\n\n\nCenter for Criminal Justice Research"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Vincenzo Palazeti",
    "section": "Education",
    "text": "Education\n\n\n MS in Applied Statistics, 2020\n\n\nUniversity of Loyola Chicago\n\n\n BS in Professional Selling, 2017\n\n\nBall State University"
  },
  {
    "objectID": "posts/survival/index.html",
    "href": "posts/survival/index.html",
    "title": "survival",
    "section": "",
    "text": "WIP\nworking on data from: https://www.frontiersin.org/articles/10.3389/fvets.2020.00388/full\nlearning survival analysis. they use SPSS “Life Table Analysis”, which from the tutorials I watched was just survival analysis.\nthe treatment was if neutered/intact, and they ran the analysis for each neutered category (I think).\nstatus is if there was a complication or not. and I am not sure what time is. time is not a specific column in the available data, unfortunately.\n\ninstall.packages(\"pammtools\")\ninstall.packages(\"readxl\")\n\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(cmdstanr)\nlibrary(survival)\nlibrary(readxl)\n\nlibrary(lubridate)\n\n\nd &lt;- read_excel('data_Golden Retriever.xlsx')\n\n# LSA, HSA, MCT, OSA\nveteran\n\nq &lt;- d |&gt; \n    mutate(\n        HSA = as.Date(as.integer(HSA), origin = \"1899-12-30\"),\n        LSA = as.Date(as.integer(LSA), origin = \"1899-12-30\"),\n        MCT = as.Date(as.integer(MCT), origin = \"1899-12-30\"),\n        OSA = as.Date(as.integer(OSA), origin = \"1899-12-30\"),\n        )\n\n# q$DOB &lt;- ymd(str_replace(q$DOB, \"UTC\", \"\"))\n\nq &lt;- q |&gt; \n    mutate(\n        time_end = NEND - DOB \n    )\n\n# q$time_end      \n# q$time_event &lt;- apply(select(q, c(LSA, HSA, MCT, OSA)), 1, FUN = min, na.rm = TRUE)\n# q$DOB\n# # ifelse(!is.na(q$time_event), q$time_event, q$time_end)\n\n\nmy_blue &lt;- rgb(45/250, 62/250, 80/250, 1)\n\nmy_theme&lt;- theme_classic() %+replace% \n           theme(panel.grid.major = element_line(),\n                 aspect.ratio = 1/1.61)\n\nvet_km &lt;- survfit(Surv(time, status) ~ strata(trt), data=veteran)\n\nbkm &lt;- broom::tidy(vet_km) %&gt;% \n       mutate(strata = str_remove(strata, 'strata\\\\(trt\\\\)=trt=')) %&gt;% \n       rename(trt=strata)\n\n\nbase_plot &lt;- bkm %&gt;% \n              ggplot(aes(time, 1-estimate, ymax=1-conf.low, ymin=1-conf.high, color=trt, fill=trt)) + \n              geom_step() + \n              pammtools::geom_stepribbon(alpha=0.5, size=0) \n\n\n\nbase_plot &lt;- bkm %&gt;% \n              ggplot(aes(time, estimate, ymax=conf.low, ymin=conf.high, color=trt, fill=trt)) + \n              geom_step() + \n              pammtools::geom_stepribbon(alpha=0.5, size=0) \n\n\n\nbase_plot +\n  my_theme + \n  labs(x='Time', y=expression(1-S(t))) +\n  scale_fill_brewer(palette = 'Set1') +\n  scale_color_brewer(palette = 'Set1') +\n  coord_cartesian(xlim=c(0, 250)) + \n  scale_y_continuous(labels = scales::percent)\n\n\n# time & status given treatment\n\n# treatment is not binary\n# treatment is whether they were neutered or not\n\n# time should be the age of the dog\n# either at \n\n# status is a complication or not"
  },
  {
    "objectID": "posts/llm/index.html",
    "href": "posts/llm/index.html",
    "title": "LLM",
    "section": "",
    "text": "WIP\ncool package instructor\nmanual indexing was beaten out of me at my first job, so when I saw this online I had to check it out.\nlooks like a cool package too.\n\nfrom dotenv import load_dotenv\nload_dotenv('../.env')\n\n\nfrom io import StringIO\nfrom typing import Annotated, Any, Iterable\nfrom openai import OpenAI\nfrom pydantic import (\n    BaseModel,\n    BeforeValidator,\n    PlainSerializer,\n    InstanceOf,\n    WithJsonSchema,\n)\nimport pandas as pd\nimport instructor\n\n\nclient = instructor.patch(OpenAI(), mode=instructor.function_calls.Mode.MD_JSON)\n\n\ndef to_markdown(df: pd.DataFrame) -&gt; str:\n    return df.to_markdown()\n\n\ndef md_to_df(data: Any) -&gt; Any:\n    if isinstance(data, str):\n        return (\n            pd.read_csv(\n                StringIO(data),  # Get rid of whitespaces\n                sep=\"|\",\n                index_col=1,\n            )\n            .dropna(axis=1, how=\"all\")\n            .iloc[1:]\n            .map(lambda x: x.strip())\n        )\n    return data\n\n\nMarkdownDataFrame = Annotated[\n    InstanceOf[pd.DataFrame],\n    BeforeValidator(md_to_df),\n    PlainSerializer(to_markdown),\n    WithJsonSchema(\n        {\n            \"type\": \"string\",\n            \"description\": \"\"\"\n                The markdown representation of the table, \n                each one should be tidy, do not try to join tables\n                that should be seperate\"\"\",\n        }\n    ),\n]\n\n\nclass Table(BaseModel):\n    caption: str\n    dataframe: MarkdownDataFrame\n\n\ndef extract_table(url: str) -&gt; Iterable[Table]:\n    return client.chat.completions.create(\n        model=\"gpt-4-vision-preview\",\n        response_model=Iterable[Table],\n        max_tokens=1800,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"\"\"Extract the table from the image, and describe it. \n                        Each table should be tidy, do not try to join tables that \n                        should be seperately described.\"\"\",\n                    },\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": url},\n                    },\n                ],\n            }\n        ],\n    )\n\n\nurl = \"https://a.storyblok.com/f/47007/2400x2000/bf383abc3c/231031_uk-ireland-in-three-charts_table_v01_b.png\"\ntables = extract_table(url)\nfor tbl in tables:\n    print(tbl.caption, end=\"\\n\")\n    print(tbl.dataframe)"
  },
  {
    "objectID": "posts/pipeline/index.html",
    "href": "posts/pipeline/index.html",
    "title": "pipeline",
    "section": "",
    "text": "import inspect\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import expit\nfrom utils import map_idx, RidgeRegTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\nCreate data\n\n# random number generator\nrng = np.random.default_rng(1995)\n\ndata = pd.DataFrame(\n    {\n        'm1_feat1': rng.normal(20, 5, 1000),\n        'm1_feat2': rng.normal(25, 15, 1000),\n        'm2_feat1': rng.normal(10, 2, 1000),\n        'm2_feat2': rng.normal(30, 10, 1000),\n        'm2_feat3': rng.normal(15, 10, 1000),        \n})\n\ndata['m1_target'] = data['m1_feat1']*10.0 + data['m1_feat1']*5.0 + rng.normal(0, 5, 1000)\ndata['m2_target'] = data['m2_feat1']*10.0 + data['m2_feat2']*5.0 + data['m2_feat3']*1.2 + rng.normal(0, 5, 1000)\n\ndata['target'] = expit(data['m1_target']*-1.10 + data['m2_target']*1.20 + rng.normal(0, 100, 1000)).round()\n\n\nPrep setup\nCreate lists which refer to the required columns. I like using the indexes rather than the names, so I convert them all to indicies. But I believe RidgeRegTransformer can take either. Check out the code here.\nThe target index needs to be the last item in the list.\n\nmodel1_features = ['m1_feat1','m1_feat2']\nmodel2_features = ['m2_feat1','m2_feat2','m2_feat3']\n\nmodel1_target = ['m1_target']\nmodel2_target = ['m2_target']\n\nmodel1_idxs = map_idx(data, model1_features) + map_idx(data, model1_target)\nmodel2_idxs = map_idx(data, model2_features) + map_idx(data, model2_target)\n\nmodel1_target_idx = len(model1_idxs) - 1\nmodel2_target_idx = len(model2_idxs) - 1\n\nmodel1_params = {\"scaler\": StandardScaler(), \"alpha\": 10}\nmodel2_params = {\"scaler\": StandardScaler(), \"alpha\": 10}\nmeta_params = {\"C\": 0.20}\n\nI am passing the StandardScaler to the regressions models via a dictionary. When the function is called the scaler is attached to the object.\nIt doesn’t feel right. I think instead the scaler should be passed in the pipeline somewhere, but then I’d have to create a sub-sub pipeline? Seems like too much\n\nTwo models with different targets and inputs\n\nmulti_model_transformer = ColumnTransformer(\n    [\n        (\n            \"model1\",\n            RidgeRegTransformer(estimator_target=model1_target_idx, **model1_params),\n            model1_idxs,\n        ),\n        (\n            \"model2\",\n            RidgeRegTransformer(estimator_target=model2_target_idx, **model2_params),\n            model2_idxs,\n        )\n    ],\n    remainder=\"drop\"\n)\n\n\nAdd meta classifier, which uses the underlying model’s predictions as input\nCan also add a passthrough to the ColumnTransformer, which will passthrough other columns from the original dataset\n\npipe = Pipeline(\n    [\n        (\"feat_transformer\", multi_model_transformer),\n        (\"meta_classifier\", LogisticRegression(**meta_params))\n    ]\n)\n\nFit the underylying ridge models & then the logistic regression\n\npipe.fit(data.values, data['target'])\n\nPipeline(steps=[('feat_transformer',\n                 ColumnTransformer(transformers=[('model1',\n                                                  RidgeRegTransformer(alpha=10,\n                                                                      estimator_target=2,\n                                                                      scaler=StandardScaler()),\n                                                  [0, 1, 5]),\n                                                 ('model2',\n                                                  RidgeRegTransformer(alpha=10,\n                                                                      estimator_target=3,\n                                                                      scaler=StandardScaler()),\n                                                  [2, 3, 4, 6])])),\n                ('meta_classifier', LogisticRegression(C=0.2))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('feat_transformer',\n                 ColumnTransformer(transformers=[('model1',\n                                                  RidgeRegTransformer(alpha=10,\n                                                                      estimator_target=2,\n                                                                      scaler=StandardScaler()),\n                                                  [0, 1, 5]),\n                                                 ('model2',\n                                                  RidgeRegTransformer(alpha=10,\n                                                                      estimator_target=3,\n                                                                      scaler=StandardScaler()),\n                                                  [2, 3, 4, 6])])),\n                ('meta_classifier', LogisticRegression(C=0.2))])feat_transformer: ColumnTransformerColumnTransformer(transformers=[('model1',\n                                 RidgeRegTransformer(alpha=10,\n                                                     estimator_target=2,\n                                                     scaler=StandardScaler()),\n                                 [0, 1, 5]),\n                                ('model2',\n                                 RidgeRegTransformer(alpha=10,\n                                                     estimator_target=3,\n                                                     scaler=StandardScaler()),\n                                 [2, 3, 4, 6])])model1[0, 1, 5]scaler: StandardScalerStandardScaler()StandardScalerStandardScaler()model2[2, 3, 4, 6]scaler: StandardScalerStandardScaler()StandardScalerStandardScaler()LogisticRegressionLogisticRegression(C=0.2)\n\n\nProduce probabilities for target\n\npipe.predict_proba(data.values)[:5]\n\narray([[0.4386381 , 0.5613619 ],\n       [0.05467618, 0.94532382],\n       [0.75733447, 0.24266553],\n       [0.23029838, 0.76970162],\n       [0.49828572, 0.50171428]])\n\n\n\nThe base pipelines can also be estimators in a meta Pipeline VotingClassifier. Something like:\nmeta_pipe = Pipeline(\n    [\n        [\n            \"meta_pipe\",\n            VotingClassifier(\n                estimators=[\n                    (\"pipe1\", multi_model_transformer1),\n                    (\"pipe2\", multi_model_transformer2),\n                    (\"pipe3\", multi_model_transformer3)\n                ],\n                voting=\"soft\",\n            ),\n        ]\n    ]\n)\n\nmeta_pipe.fit(data.values, data[\"target\"])"
  },
  {
    "objectID": "posts/docker/index.html",
    "href": "posts/docker/index.html",
    "title": "docker",
    "section": "",
    "text": "Two stage Dockerfile. Runner image is slim with only virtual environment.\nFROM ubuntu:20.04 AS builder-image\n\n# avoid stuck build due to user prompt\nARG DEBIAN_FRONTEND=noninteractive\n\nRUN apt-get update && \\\n    apt-get install --no-install-recommends -y python3.9 python3.9-dev python3.9-venv python3-pip python3-wheel build-essential && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\n# create and activate virtual environment\n# using final folder name to avoid path issues with packages\nRUN python3.9 -m venv /home/myuser/venv\nENV PATH=\"/home/myuser/venv/bin:$PATH\"\n\n# install requirements\nCOPY app/requirements.txt .\nRUN pip3 install --no-cache-dir wheel\nRUN pip3 install --no-cache-dir -r requirements.txt\n\nFROM ubuntu:20.04 AS runner-image\nRUN apt-get update && \\\n    apt-get install --no-install-recommends -y python3.9 python3-venv && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\nRUN useradd --create-home myuser\nCOPY --from=builder-image /home/myuser/venv /home/myuser/venv\n\nUSER myuser\nRUN mkdir /home/myuser/code\nWORKDIR /home/myuser/code\nCOPY app .\n\n# make sure all messages always reach console\nENV PYTHONUNBUFFERED=1\n\n# activate virtual environment\nENV VIRTUAL_ENV=/home/myuser/venv\nENV PATH=\"/home/myuser/venv/bin:$PATH\"\nI’ve been told installing python manually is not worth the effort & nudged to use the official docker python images.\nThis is good advice, because I have struggled with C package installs (specifically for LGBM).\n\nCreate a container & attach a volume to the image. This command opens the container in interactive mode, mounts the /app directory as a volumne, and links the port 8080.\nThis is useful because changes to application, or whatever you are working on, are reflected inside of the docker container.\nI have been using this with streamlit, but I believe it should work with jupyter. Linking to jupyter through a docker container is a pain, so maybe not. I’ll have to check.\ndocker run -it --rm -v $(pwd)/app:/home/myuser/code -p 8080:8080 img_name\n\nIf your environment requires variables you can pass them through with --env-file\ndocker run --env-file .env-local \nWith the format:\nDS_BUCKET=XXXX\nAWS_ACCESS_KEY_ID_DEV=XXXX\nAWS_SECRET_ACCESS_KEY_DEV=XXXX"
  },
  {
    "objectID": "posts/airflow/index.html",
    "href": "posts/airflow/index.html",
    "title": "airflow",
    "section": "",
    "text": "this dag has an expanding training window with a maximum amount.\nwe found that three years worth of data was perferable, and anything further would degrade performance.\nalso, the backtest needed to start in 2011, which was only 1 year after the training data began.\nso, for the first two years the training data window would expand to a maximum of three years, then retain only the most recent three years.\n\nwhat makes this difficult is airflow does not allow python in the dag.\nto get around this, I wrote the functions outside the dag & used the built in airflow utilities for date manipulation & function execution.\nPythonOperator grabbed the functions and macros.ds_add() manipulated the training & prediction windows\nthese windows were set here in the file, but I probably shoud’ve used a CLI like click\nfrom datetime import datetime, timedelta\nimport pendulum\nfrom airflow import DAG\nfrom airflow.models import Variable\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators.python import PythonOperator\nfrom airflow.providers.docker.operators.docker import DockerOperator\nfrom docker.types import Mount\n\ndefault_arguments = {\n    \"owner\": \"airflow\",\n    \"depends_on_past\": True,\n    \"retries\": 0,\n}\n\ndef check_start_date(**kwargs):\n    date_given = kwargs['date']\n    if date_given &gt; '2010-01-01':\n        return date_given\n    else:\n        return '2010-01-01'\n\ndef check_end_date(**kwargs):\n    date_given = kwargs['date']\n    if date_given &lt; '2020-12-31':\n        return date_given\n    else:\n        return '2020-12-31'\n\nwith DAG(\n    dag_id=\"backfiill data_pull\",\n    description=\"Backfill DAG for data pulls\",\n    schedule_interval=timedelta(days=100),\n    start_date = pendulum.datetime(2011, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    default_args=default_arguments,\n) as dag:\n\n\n    format_start_date = PythonOperator(\n        task_id='format_start_date',\n        python_callable=check_start_date,\n        op_kwargs={\"date\":\"{{ macros.ds_add(ds, -1096) }}\"},\n    )\n    format_end_date = PythonOperator(\n        task_id='format_end_date',\n        python_callable=check_end_date,\n        op_kwargs={\"date\":\"{{ macros.ds_add(ds, 99) }}\"},\n    )\n\n    training_date_start =  \"{{ ti.xcom_pull(task_ids='format_start_date') }}\"\n    training_date_end = \"{{ macros.ds_add(ds, -1) }}\"\n\n    prediction_period_start = \"{{ ds }}\"\n    prediction_period_end =  \"{{ ti.xcom_pull(task_ids='format_end_date') }}\"\n\n\n    docker_url = Variable.get(\"DOCKER_URL\", deserialize_json=True)\n    aws_user = Variable.get(\"AWS_USERNAME\", deserialize_json=True)\n\n    training_data_pull = DockerOperator(\n        task_id=\"training_data_pull\",\n        image=\"image_name:latest\",\n        command=f\"python scripts/data_pull.py \\\n            --aws_user {aws_user} \\\n            --start_date {training_date_start} \\\n            --end_date {training_date_end}\",\n        network_mode='host',\n        docker_url=docker_url,\n        auto_remove=True,\n        mounts=[\n            Mount(target='/home/myuser/.aws', source='/home/airflow/.aws', type='bind'),\n            Mount(target='/home/myuser/code/scripts', source='/home/airflow/projects', type='bind')\n            ],\n        dag=dag\n    )\n\n    prediction_data_pull = DockerOperator(\n        task_id=\"prediction_data_pull\",\n        image=\"image_name:latest\",\n        command=f\"python scripts/data_pull.py \\\n            --aws_user {aws_user} \\\n            --start_date {prediction_period_start} \\\n            --end_date {prediction_period_end}\",\n        network_mode='host',\n        docker_url=docker_url,\n        auto_remove=True,\n        mounts=[\n            Mount(target='/home/myuser/.aws', source='/home/airflow/.aws', type='bind'),\n            Mount(target='/home/myuser/code/scripts', source='/home/airflow/projects', type='bind')\n            ],\n        dag=dag\n    )\n\n    format_start_date &gt;&gt; format_end_date &gt;&gt; training_data_pull &gt;&gt; prediction_data_pull"
  },
  {
    "objectID": "posts/pandas/index.html",
    "href": "posts/pandas/index.html",
    "title": "pandas",
    "section": "",
    "text": "Useful Pandas\nGrab all of the month names\nmonths = pd.date_range('2020-01-01','2020-12-31',freq='MS').map(lambda x: x.month_name())"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\nsurvival\n\n\nDec 29, 2023\n\n\n\n\npipeline\n\n\nDec 27, 2023\n\n\n\n\npandas\n\n\nDec 20, 2023\n\n\n\n\nLLM\n\n\nDec 16, 2023\n\n\n\n\ndocker\n\n\nDec 2, 2023\n\n\n\n\nairflow\n\n\nDec 2, 2023\n\n\n\n\nOLS\n\n\nNov 29, 2023\n\n\n\n\n\nNo matching items"
  }
]